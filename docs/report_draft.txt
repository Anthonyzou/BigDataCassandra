Big Data Project Report 

Authors:    Shenwei Liao, Anthony Ou, Jacqueline Terlaan
Class:      CMPUT 391
Term:       Winter 2014

Section 0: Introduction to Cassandra 

    -What is NoSQL?
        -"Not Only SQL". But what does this mean?
            -Any database system which does not strictly conform to the
             relational database model. We may have any one of the following:
                -Does not meet all "ACID" properties.
                -May store data in a fashion other than a strictly
                 relational set of tables.

    -Why NoSQL?
        -We have some BIG data to deal with (several TB), and growing!
        -Big Data implies distribution;
        -Distribution implies CAP theorem;
        -CAP theorem implies compromises!

    -What is Apache Project's Cassandra?
        -NoSQL DBMS, meant to be highly scalable.
        -FOSS/FLOSS alternative to proprietary (and expensive) Oracle
         NoSQL systems.

        Features:
            -Available, Partition-tolerant
            -Eventually consistent
            -High scalability
            -"Schemaless" (implicit schema)
            -Has its own query language: Apache cql
            -Data compaction

    -Why Cassandra?
        -Partition-tolerant (essential for clustered systems).
        -Available: mirrors "real-world" constraints on call-detail record
         systems.
        -Eventual consistency is an acceptable compromise; we are not
         running a mission-critical system (like a bank), and most of our
         data insertion is similar to logging data.
        -Cassandra is especially well-suited to storing logs.
        -Widely used and relatively bug-free, especially for such "new" 
         software.
        -Handles map-reduce and low-level details of storage process for nodes,
         as well as performing maintenance (as specified by DB admin), such
         as compaction.
        -Very low chance of data loss (except if the DBA makes a mistake),
         even when upgrading Cassandra.

Section 1: Goals and Objectives
    -To successfully implement a distributed database system which will
     reliably store at least 16TB of random data consisting of random
     numeric and string-like data types organized into large tables with
     many columns (attributes).
    -To distribute the data in the above tables across 8 "nodes" (servers), 
     which form our "cluster".
    -To generate additional tables distributed throughout the cluster to
     be optimally configured for the queries to meet their own requirements
     (see below).
     
    -To query the above tables such that all of the following requirements
     are met by five queries.
        1) Four of five queries must retrieve data across from all of the
           distributed nodes.
        2) At least one of the queries must contain at least ten atomic
           conditions (formulas) in the WHERE clause.
        3) At least two of the queries must utilize both the GROUP BY and
           ORDER BY clauses. Since Cassandra does not yet support GROUP BY
           aggregation, the same functionality must be acheived in another
           way.
        4) At least three of the five queries must be range queries which
           specify an upper and lower bound on some ordered attribute that
           is used for the purposes of querying.
        5) None of the five queries can be trivial. That is, there can be
           no simple key searches or anything that is of little practical
           interest for measuring how a database system can deal with
           challenging or expensive requests.

    -To record the execution time of each query.

Section 2: Methodology, Tools and Equipment Used
    -8 server instances, each with:
        -32GB virtual memory.
        -1TB secondary storage (hard-disk space) per node.
        
    -

Section 3: Implementation

Subsection 3.1: Data Generation
    -Cluster name: 'group3'
        -Only nodes with this name can join the cluser.

    -Shell script
        -To run: sh group3.sh
        -Set up to be able to run multiple instances and then send the output
         to an output file.
        -To generate data:

    -Table schema
        -"CDR"
            -Exactly 100 columns.
            -Numbers range from 0 to 9.9 million for all integers.
            -Has 10 column keys and 1 partition key.
        -"QUERY3"
            -Contains the same data, but the column keys are ordered MSC_CODE first in
             an attempt to optimize for range-queries based on values of those column keys.
        -"GROUP_BY_MONTH" 
            -Keys are between 1 and 31.
            -Reduces data to facilitate completion of a query equivalent to one
             in SQL containing 'GROUP BY MONTH_DAY'.
        -"GOUP_BY_MOBILE_ID_TYPE"
            -Keys are between 0 and 7.
            -Similar to "GROUP_BY_MONTH", but instead the assignment of the rows' locations
             is based on its insertion number modulo 8.

    -Keyspaces
        -One keyspace used, with a 'SimpleStrategy' class and replication_factor of 1.
            -No backups or snapshots are automatically made by the server.

    -Python 2.7 scripts
        -generate.py
            -Begins with attempting to establish a connection to the local machine which is part
             of the cluster.
            -Expects program arguments: 
                -The number of days to generate data for. 
                -Different seed values to prevent collisions of data insertions.
                    -Cassandra has no support for primary keys; collisions result in overwriting data.
                    -Allows for multiple concurrent instances of data generation.
                -Without seed argument, the script will use its default seed value (typically used
                 for consistency in generation of data) and will drop all of the data before completely
                 regenerating it.
                -Depending on the results from stat.py (see below), 
            -Asynchronously executes the insertion statements to prevent blocking on the script.
            -generate() function
                -Allows for custom ranges on specified columns.
                -Allows for custom data for specified data types.

        -stat.py (mainly for testing)
            -Reads in the contents of "partial_data_table.txt" and "cdr_table.txt", then 
             counts the number of non-empty columns for each row.
            -Prints the resulting number (frequency) of items for each column.



Subsection 3.2: CQL Queries 

    1)
SELECT count (*) as ten_atomic
FROM cdr 
WHERE 
(CITY_ID,SERVICE_NODE_ID,RUM_DATA_NUM,MONTH_DAY,DUP_SEQ_NUM,MOBILE_ID_TYPE,SEIZ_CELL_NUM,FLOW_DATA_INC,SUB_HOME_INT_PRI,CON_OHM_NUM) 
> (10000,10000,10000,3,10000,1,10000,10000,10000,10000)
AND (CITY_ID,SERVICE_NODE_ID,RUM_DATA_NUM,MONTH_DAY,DUP_SEQ_NUM,MOBILE_ID_TYPE,SEIZ_CELL_NUM,FLOW_DATA_INC,SUB_HOME_INT_PRI,CON_OHM_NUM) 
< (150000,150000,150000,30,150000,8,150000,150000,150000,150000)
LIMIT 40000000
ALLOW FILTERING ;

           Cluster-wide range query with 10 atomic formulas. Searches on every column key of the
           CDR table, as this is the only conceivable way to properly execute a range query.
           Returns results from each node, satisfies the range query requirement and has 10 atomic
           formulas.

           Potential uses: many 'outliers' are eliminated from the selection because they are above
           and below the minimum and maximum values of all the columns respectively, by approximately
           10%. In other words, any 'extreme' data points in the CDR table can be ignored for
           statistical analysis.

        2)
SELECT count (*) as range_city_id
FROM cdr
WHERE
CITY_ID > 5000 AND CITY_ID < 90000
LIMIT 40000000
ALLOW FILTERING;

           Satisfies two requirements: cluster-wide and range query. Retrieves the number of
           cdr entries for cities with IDs strictly greater than 5000 and strictly less than
           90000. Limits the maximal number of elements counted to 40000000. Filtering of
           results is enabled.

           Potential uses: In some cases companies may wish to know how many calls were made
           in a given range of cities, based on certain CITY_IDs. While those cities may not
           have any major features in common, this query can be used in cases where only a small
           sample of all cities is required.

        3) 
SELECT count(*) as range_DUP_SEQ_NUM
FROM cdr
WHERE 
(CITY_ID,SERVICE_NODE_ID,RUM_DATA_NUM,MONTH_DAY,DUP_SEQ_NUM)
>(0,0,0,0,30000) AND
(CITY_ID,SERVICE_NODE_ID,RUM_DATA_NUM,MONTH_DAY,DUP_SEQ_NUM)
<(9900000,9900000,9900000,9900000,300000)
LIMIT 40000000
ALLOW FILTERING;

           Satsifies two requirements: cluster-wide and range query. Counts the number of cdr
           entries such that DUP_SEQ_NUM is strictly greater than 30000 and strictly less than
           300000. All other atomic conditions in the range query are used to ensure that
           all of the results fall within a valid range and have no null-valued entries for
           CITY_ID, SERVICE_NODE_ID, and so on. Because we are trying to query mainly by 
           DUP_SEQ_NUM, we allow filtering on the query in order to speed up execution.

           Potential uses: Suppose that statistical data is to be generated for rows in the
           CDR table for which there are no null-valued entries in the columns CITY_ID,
           SERVICE_NODE_ID, RUM_DATA_NUM, MONTH_DAY and DUP_SEQ_NUM, and that DUP_SEQ_NUM
           falls within a more specific range, that is 30000-300000 (not inclusive). The
           records could then be analyzed and compared to data for which not all of the
           attributes above are null, to compare behaviors of callers with different amounts
           of privacy. This could address questions such as "do people with more privacy make
           certain types of calls compared to people who do not?".

        4)
SELECT MOBILE_ID_TYPE,count
from GROUP_BY_MOBILE_ID_TYPE

           Satisfies two requirements: group-by + order-by clause, and cluster-wide query.

           A table of counters aggregates the keys of the MOBILE_ID_TYPE columns with the 
           count of each number of rows in the CDR table with matchine MOBILE_ID_TYPE.
           Ordering by mobile_id_type is acheived by using a the 'USE CLUSTERING ORDER' 
           clause in the data definition statement in generate.py.
           We then retrieve GROUP_BY_MOBILE_ID_TYPEs contents and the count of its rows
           in order of appearance, i.e., the order in which the aggregated rows are sorted.

           MOBILE_ID is the insertion number modulo 8 (or however many nodes), therefore
           the rows of the MOBILE_ID table are evenly distributed throughout the cluster.
           This means that we can easily use this query as a check for correctness of
           the insertion process, as counting the number of rows in the MOBILE_ID_TYPE table
           on each node should total up to the number of unique items in the database.

           This is an atomic, yet seemingly asynchronous transaction. In other words, only
           small critical sections of execution are protected for short amounts of time
           to prevent collisions, but otherwise parallelism is fully exploited (with minimal
           constraints of Ahmdal's Law) due to the very even distribution of data across all 
           8 nodes. 

           Updating counts is done in the following way:

update group_by_MOBILE_ID_TYPE set count = count + 1  where MOBILE_ID_TYPE = ? and id = 1

           Potential uses: Most likely for the generation of histograms, but other basic
           kinds of statistical analysis can be performed on these small MOBILE_ID_TYPE 
           'buckets' as well, such as calculating arithmetic averages, etc.

        5) 
SELECT month_day, count FROM group_by_month

           Satisfies the group-by + order-by clause requirements. 
           'CLUSTERING ORDER BY (MONTH_DAY)' ensures that the GROUP_BY_MONTHs entries are ordered
           by MONTH_DAY, and GROUP BY functionality is acheived via 

           The rows in GROUP_BY_MONTH are distributed in a pseudorandom fashion throughout the
           cluster.

           Potential uses:


Section 4: Theoretical Description
    -Obviously, we are going to be "using" the CAP theorem.
    -As explained above, Cassandra is high-availability, partition-tolerant
     and eventually consistent.
    
    -To get the equivalent of a GROUP BY aggregation:
        -Recall: a GROUP BY aggregation is basically a function which
         gathers rows of data retrieved by a query and 'clusters' them
         together based on the attribute that we are grouping by.

        -Suppose we have a table T on which we have columns C1,...,Cn,
         with rows R1,...,Rm. Say that P(C1) is an atomic formula which
         is satisfied for some C1 columns in R1,...,Rm.

            SELECT C1, Ci
            FROM T
            WHERE P(C1)
            GROUP BY Ci;    -- Where 1 <= i <= n

        -The above query will return something like this:
            
             _______________
            |   C1  |   Ci  |
            +-------+-------+


Section 5: Test Results
    -Insertion of data for the first test query: 
        -Amount of data written: 300k entries. 
        -Time taken: approximately 35 minutes.

    -The amount of time required to complete the creation of the table 
     and insertion of data into that table for the first query took as long
     as it did for two main reasons:
        1) In traditional relational database systems aggregate functions
           such as the 'GROUP BY' clause are explicitly supported.
           While Cassandra does support certain aggregation clauses, such
           as 'ORDER BY', there is no explicit 'GROUP BY' function supported
           for use in (search) queries. Because of this, tables must be 
           created with aggregation done beforehand.
           Hence, anything that could be considered 'overhead' as a result of
           using a GROUP BY clause in a traditional relational database query
           is incurred in the generation and population of data tables in
           Cassandra. Hence, it took longer to insert data as Cassandra was
           essentially used to 'pre-proccess'/'pre-aggregate' it on insertion,
           as opposed to performing the equivalent aggregation during the
           execution of search queries where GROUP BY is desired.

           However, it is much faster.

        2) Counter tables were also required.

Section 6: Experimental Results
    -

Section 7: Conclusions

Section 8: Notes on Possible Improvements

Section 9: References Used
