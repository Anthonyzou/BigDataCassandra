Big Data Project Report 

Authors:    Shenwei Liao, Anthony Ou, Jacqueline Terlaan
Class:      CMPUT 391
Term:       Winter 2014

Section 0: Introduction to Cassandra 

    -What is NoSQL?
        -"Not Only SQL". But what does this mean?
            -Any database system which does not strictly conform to the
             relational database model. We may have any one of the following:
                -Does not meet all "ACID" properties.
                -May store data in a fashion other than a strictly
                 relational set of tables.

    -Why NoSQL?
        -We have some BIG data to deal with (several TB), and growing!
        -Big Data implies distribution;
        -Distribution implies CAP theorem;
        -CAP theorem implies compromises!

    -What is Apache Project's Cassandra?
        -NoSQL DBMS, meant to be highly scalable.
        -FOSS/FLOSS alternative to proprietary (and expensive) Oracle
         NoSQL systems.

        Features:
            -Available, Partition-tolerant
            -Eventually consistent
            -High scalability
            -"Schemaless" (implicit schema)
            -Has its own query language: Apache cql
            -Data compaction

    -Why Cassandra?
        -Partition-tolerant (essential for clustered systems).
        -Available: mirrors "real-world" constraints on call-detail record
         systems.
        -Eventual consistency is an acceptable compromise; we are not
         running a mission-critical system (like a bank), and most of our
         data insertion is similar to logging data.
        -Cassandra is especially well-suited to storing logs.
        -Widely used and relatively bug-free, especially for such "new" 
         software.
        -Handles map-reduce and low-level details of storage process for nodes,
         as well as performing maintenance (as specified by DB admin), such
         as compaction.
        -Very low chance of data loss (except if the DBA makes a mistake),
         even when upgrading Cassandra.

Section 1: Goals and Objectives
    -To successfully implement a distributed database system which will
     reliably store at least 16TB of random data consisting of random
     numeric and string-like data types organized into large tables with
     many columns (attributes).
    -To distribute the data in the above tables across 8 "nodes" (servers), 
     which form our "cluster".
    -To generate additional tables distributed throughout the cluster to
     be optimally configured for the queries to meet their own requirements
     (see below).
     
    -To query the above tables such that all of the following requirements
     are met by five queries.
        1) Four of five queries must retrieve data across from all of the
           distributed nodes.
        2) At least one of the queries must contain at least ten atomic
           conditions (formulas) in the WHERE clause.
        3) At least two of the queries must utilize both the GROUP BY and
           ORDER BY clauses. Since Cassandra does not yet support GROUP BY
           aggregation, the same functionality must be acheived in another
           way.
        4) At least three of the five queries must be range queries which
           specify an upper and lower bound on some ordered attribute that
           is used for the purposes of querying.
        5) None of the five queries can be trivial. That is, there can be
           no simple key searches or anything that is of little practical
           interest for measuring how a database system can deal with
           challenging or expensive requests.

    -To record the execution time of each query.

Section 2: Methodology, Tools and Equipment Used
    -8 server instances, each with:
        -32GB virtual memory.
        -1TB secondary storage (hard-disk space) per node.
        
    -

Section 3: Implementation

Subsection 3.1: Data Generation
    -Cluster name: 'group3'
        -Only nodes with this name can join the cluser.

    -Shell script
        -To run: sh group3.sh
        -Set up to be able to run multiple instances and then send the output
         to an output file.
        -To generate data:

    -Table schema
        -"CDR"
            -Exactly 100 columns.
            -Numbers range from 0 to 9.9 million for all integers.
            -Has 10 column keys and 1 partition key.
        -"QUERY3"
            -Contains the same data, but the column keys are ordered MSC_CODE first in
             an attempt to optimize for range-queries based on values of those column keys.
        -"GROUP_BY_MONTH" 
            -Keys are between 1 and 31.
            -Reduces data to facilitate completion of a query equivalent to one
             in SQL containing 'GROUP BY MONTH_DAY'.
        -"GOUP_BY_MOBILE_ID_TYPE"
            -Keys are between 0 and 7.
            -Similar to "GROUP_BY_MONTH", but instead the assignment of the rows' locations
             is based on its insertion number modulo 8.

    -Keyspaces
        -One keyspace used, with a 'SimpleStrategy' class and replication_factor of 1.
            -No backups or snapshots are automatically made by the server.

    -Python 2.7 scripts
        -generate.py
            -Begins with attempting to establish a connection to the local machine which is part
             of the cluster.
            -Expects program arguments: 
                -The number of days to generate data for. 
                -Different seed values to prevent collisions of data insertions.
                    -Cassandra has no support for primary keys; collisions result in overwriting data.
                    -Allows for multiple concurrent instances of data generation.
                -Without seed argument, the script will use its default seed value (typically used
                 for consistency in generation of data) and will drop all of the data before completely
                 regenerating it.
                -Depending on the results from stat.py
        -stat.py
            -Reads in the contents of "partial_data_table.txt" and counts the number of non-empty
             columns for each row.



Subsection 3.2: CQL Queries 

    1)
SELECT count (*) as ten_atomic
FROM cdr 
WHERE 
(CITY_ID,SERVICE_NODE_ID,RUM_DATA_NUM,MONTH_DAY,DUP_SEQ_NUM,MOBILE_ID_TYPE,SEIZ_CELL_NUM,FLOW_DATA_INC,SUB_HOME_INT_PRI,CON_OHM_NUM) 
> (10000,10000,10000,3,10000,1,10000,10000,10000,10000)
AND (CITY_ID,SERVICE_NODE_ID,RUM_DATA_NUM,MONTH_DAY,DUP_SEQ_NUM,MOBILE_ID_TYPE,SEIZ_CELL_NUM,FLOW_DATA_INC,SUB_HOME_INT_PRI,CON_OHM_NUM) 
< (150000,150000,150000,30,150000,8,150000,150000,150000,150000)
LIMIT 40000000
ALLOW FILTERING ;
           Cluster-wide range query with 10 atomic formulas. Searches on every column key of the
           CDR table, as this is the only conceivable way to properly execute a range query.
           Returns results from each node, satisfies the range query requirement and has 10 atomic
           formulas.

           Potential uses: many 'outliers' are eliminated from the selection because they are above
           and below the minimum and maximum values of all the columns respectively, by approximately
           10%. In other words, any 'extreme' data points in the CDR table can be ignored for
           statistical analysis.
        2)
SELECT count (*) as range_city_id
FROM cdr
WHERE
CITY_ID > 5000 AND CITY_ID < 90000
LIMIT 40000000
ALLOW FILTERING;
           Cluster-wide range query.
        3)
SELECT count(*) as range_DUP_SEQ_NUM
FROM cdr
WHERE 
(CITY_ID,SERVICE_NODE_ID,RUM_DATA_NUM,MONTH_DAY,DUP_SEQ_NUM)
>(0,0,0,0,30000) AND
(CITY_ID,SERVICE_NODE_ID,RUM_DATA_NUM,MONTH_DAY,DUP_SEQ_NUM)
<(9900000,9900000,9900000,9900000,300000)
LIMIT 40000000
ALLOW FILTERING;
           Because we are trying to query by DUP_SEQ_NUM, we must allow filtering on the query.
           In order to speed up execution
        4)
SELECT MOBILE_ID_TYPE,count
from GROUP_BY_MOBILE_ID_TYPE

           A table of counters aggregates the keys of the MOBILE_ID_TYPE columns with the 

           Example:
update group_by_MOBILE_ID_TYPE set count = count + 1  where MOBILE_ID_TYPE = ? and id = 1

           This is an atomic, yet seemingly asynchronous transaction

           MOBILE_ID is the insertion number modulo 8 (or however many nodes), therefore
           the rows of the MOBILE_ID table are evenly distributed throughout the cluster.
           This means that we can easily use this query as a check for correctness of
           the insertion process, as counting the number of rows in the MOBILE_ID_TYPE table
           on each node should total up to the number of unique items in the database.

        5) 
SELECT month_day, count FROM group_by_month
           The rows in GROUP_BY_MONTH are distributed in a pseudorandom fashion throughout the
           cluster.

    -Python data generation
    -Logging software

Section 4: Theoretical Description
    -Obviously, we are going to be "using" the CAP theorem.
    -As explained above, Cassandra is high-availability, partition-tolerant
     and eventually consistent.
    
    -To get the equivalent of a GROUP BY aggregation:
        -Recall: a GROUP BY aggregation is basically a function which
         gathers rows of data retrieved by a query and 'clusters' them
         together based on the attribute that we are grouping by.

        -Suppose we have a table T on which we have columns C1,...,Cn,
         with rows R1,...,Rm. Say that P(C1) is an atomic formula which
         is satisfied for some C1 columns in R1,...,Rm.

            SELECT C1, Ci
            FROM T
            WHERE P(C1)
            GROUP BY Ci;    -- Where 1 <= i <= n

        -The above query will return something like this:
            
             _______________
            |   C1  |   Ci  |
            +-------+-------+


Section 5: Test Results
    -Insertion of data for the first test query: 
        -Amount of data written: 300k entries. 
        -Time taken: approximately 35 minutes.

    -The amount of time required to complete the creation of the table 
     and insertion of data into that table for the first query took as long
     as it did for two main reasons:
        1) In traditional relational database systems aggregate functions
           such as the 'GROUP BY' clause are explicitly supported.
           While Cassandra does support certain aggregation clauses, such
           as 'ORDER BY', there is no explicit 'GROUP BY' function supported
           for use in (search) queries. Because of this, tables must be 
           created with aggregation done beforehand.
           Hence, anything that could be considered 'overhead' as a result of
           using a GROUP BY clause in a traditional relational database query
           is incurred in the generation and population of data tables in
           Cassandra. Hence, it took longer to insert data as Cassandra was
           essentially used to 'pre-proccess'/'pre-aggregate' it on insertion,
           as opposed to performing the equivalent aggregation during the
           execution of search queries where GROUP BY is desired.

           However, it is much faster.

        2) Counter tables were also required.

Section 6: Experimental Results
    -

Section 7: Conclusions

Section 8: Notes on Possible Improvements

Section 9: References Used
